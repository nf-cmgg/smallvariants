{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"nf-cmgg/smallvariants","text":""},{"location":"#introduction","title":"Introduction","text":"<p>nf-cmgg/smallvariants is a nextflow pipeline for calling and annotating small variants from short DNA reads for WES and WGS data.</p> <p>The pipeline is built using Nextflow, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The Nextflow DSL2 implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from nf-core/modules in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!</p>"},{"location":"#pipeline-summary","title":"Pipeline summary","text":"<p>Note</p> <p>If you are new to Nextflow and nf-core, please refer to this page on how to set-up Nextflow. Make sure to test your setup with <code>-profile test</code> before running the workflow on actual data.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li>Install <code>Nextflow</code> (<code>&gt;=24.10.0</code>)</li> <li>Install any of <code>Docker</code>, <code>Singularity</code> (you can follow this tutorial), <code>Podman</code>, <code>Shifter</code> or <code>Charliecloud</code> for full pipeline reproducibility (you can use <code>Conda</code> both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see docs).</li> </ol> samplesheet.csv<pre><code>sample,family,cram,crai\nSAMPLE_1,FAMILY_1,SAMPLE_1.cram,SAMPLE_1.crai\n</code></pre> <p>Each row represents a single sample to be analysed. More information can be found in the usage documentation.</p> <p>Now, you can run the pipeline using:</p> <pre><code>nextflow run nf-cmgg/smallvariants --input samplesheet.csv --outdir &lt;OUTDIR&gt; --genome GRCh38 -profile &lt;docker/singularity/podman/shifter/charliecloud/conda/institute&gt;\n</code></pre> <p>This pipeline contains a lot of parameters to customize your pipeline run. Please take a look at the parameters documentation for an overview.</p> <p>Warning</p> <p>Please provide pipeline parameters via the CLI or Nextflow <code>-params-file</code> option. Custom config files including those provided by the <code>-c</code> Nextflow option can be used to provide any configuration except for parameters; see docs.</p>"},{"location":"#credits","title":"Credits","text":"<p>nf-cmgg/smallvariants was originally written and is maintained by @nvnieuwk.</p> <p>Special thanks to @matthdsm for the many tips and feedback and to @mvheetve and @ToonRossel for testing the pipeline.</p>"},{"location":"#contributions-and-support","title":"Contributions and Support","text":"<p>If you would like to contribute to this pipeline, please see the contributing guidelines.</p>"},{"location":"#citations","title":"Citations","text":"<p>An extensive list of references can be found in the <code>CITATIONS</code> section.</p>"},{"location":"CITATIONS/","title":"nf-cmgg/smallvariants: Citations","text":""},{"location":"CITATIONS/#nf-core","title":"nf-core","text":"<p>Ewels PA, Peltzer A, Fillinger S, Patel H, Alneberg J, Wilm A, Garcia MU, Di Tommaso P, Nahnsen S. The nf-core framework for community-curated bioinformatics pipelines. Nat Biotechnol. 2020 Mar;38(3):276-278. doi: 10.1038/s41587-020-0439-x. PubMed PMID: 32055031.</p>"},{"location":"CITATIONS/#nextflow","title":"Nextflow","text":"<p>Di Tommaso P, Chatzou M, Floden EW, Barja PP, Palumbo E, Notredame C. Nextflow enables reproducible computational workflows. Nat Biotechnol. 2017 Apr 11;35(4):316-319. doi: 10.1038/nbt.3820. PubMed PMID: 28398311.</p>"},{"location":"CITATIONS/#software-packagingcontainerisation-tools","title":"Software packaging/containerisation tools","text":"<ul> <li>Anaconda</li> </ul> <p>Anaconda Software Distribution. Computer software. Vers. 2-2.4.0. Anaconda, Nov. 2016. Web.</p> <ul> <li>Bioconda</li> </ul> <p>Gr\u00fcning B, Dale R, Sj\u00f6din A, Chapman BA, Rowe J, Tomkins-Tinch CH, Valieris R, K\u00f6ster J; Bioconda Team. Bioconda: sustainable and comprehensive software distribution for the life sciences. Nat Methods. 2018 Jul;15(7):475-476. doi: 10.1038/s41592-018-0046-7. PubMed PMID: 29967506.</p> <ul> <li>BioContainers</li> </ul> <p>da Veiga Leprevost F, Gr\u00fcning B, Aflitos SA, R\u00f6st HL, Uszkoreit J, Barsnes H, Vaudel M, Moreno P, Gatto L, Weber J, Bai M, Jimenez RC, Sachsenberg T, Pfeuffer J, Alvarez RV, Griss J, Nesvizhskii AI, Perez-Riverol Y. BioContainers: an open-source and community-driven framework for software standardization. Bioinformatics. 2017 Aug 15;33(16):2580-2582. doi: 10.1093/bioinformatics/btx192. PubMed PMID: 28379341; PubMed Central PMCID: PMC5870671.</p> <ul> <li>Docker</li> </ul> <p>Merkel, D. (2014). Docker: lightweight linux containers for consistent development and deployment. Linux Journal, 2014(239), 2. doi: 10.5555/2600239.2600241.</p> <ul> <li>Singularity</li> </ul> <p>Kurtzer GM, Sochat V, Bauer MW. Singularity: Scientific containers for mobility of compute. PLoS One. 2017 May 11;12(5):e0177459. doi: 10.1371/journal.pone.0177459. eCollection 2017. PubMed PMID: 28494014; PubMed Central PMCID: PMC5426675.</p>"},{"location":"CITATIONS/#pipeline-tools-in-alphabetical-order","title":"Pipeline tools (in alphabetical order)","text":"<ul> <li>BCFTools</li> </ul> <p>Li H: A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data. Bioinformatics. 2011 Nov 1;27(21):2987-93. doi: 10.1093/bioinformatics/btr509. PubMed PMID: 21903627; PubMed Central PMCID: PMC3198575.</p> <ul> <li>Bedtools</li> </ul> <p>Quinlan AR and Hall IM, 2010. BEDTools: a flexible suite of utilities for comparing genomic features. Bioinformatics. 26, 6, pp. 841\u2013842.</p> <ul> <li>elPrep</li> </ul> <p>Herzeel C, Costanza P, Decap D, Fostier J, Wuyts R, Verachtert W (2021) Multithreaded variant calling in elPrep 5. PLoS ONE 16(2): e0244471</p> <ul> <li>EnsemblVEP</li> </ul> <p>McLaren W, Gil L, Hunt SE, et al.: The Ensembl Variant Effect Predictor. Genome Biol. 2016 Jun 6;17(1):122. doi: 10.1186/s13059-016-0974-4. PubMed PMID: 27268795; PubMed Central PMCID: PMC4893825.</p> <ul> <li>GATK</li> </ul> <p>McKenna A, Hanna M, Banks E, et al.: The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data. Genome Res. 2010 Sep;20(9):1297-303. doi: 10.1101/gr.107524.110. Epub 2010 Jul 19. PubMed PMID: 20644199; PubMed Central PMCID: PMC2928508.</p> <ul> <li> <p>Gawk</p> </li> <li> <p>GNU tar</p> </li> <li> <p>Mosdepth</p> </li> </ul> <p>Brent S Pedersen, Aaron R Quinlan, Mosdepth: quick coverage calculation for genomes and exomes, Bioinformatics, Volume 34, Issue 5, 01 March 2018, Pages 867\u2013868. doi: 10.1093/bioinformatics/btx699. PubMed PMID: 29096012. PubMed Central PMCID: PMC6030888.</p> <ul> <li>MultiQC</li> </ul> <p>Ewels P, Magnusson M, Lundin S, K\u00e4ller M. MultiQC: summarize analysis results for multiple tools and samples in a single report. Bioinformatics. 2016 Oct 1;32(19):3047-8. doi: 10.1093/bioinformatics/btw354. Epub 2016 Jun 16. PubMed PMID: 27312411. PubMed Central PMCID: PMC5039924.</p> <ul> <li> <p>RTG Tools</p> </li> <li> <p>SAMtools</p> </li> </ul> <p>Li H, Handsaker B, Wysoker A, Fennell T, Ruan J, Homer N, Marth G, Abecasis G, Durbin R; 1000 Genome Project Data Processing Subgroup. The Sequence Alignment/Map format and SAMtools. Bioinformatics. 2009 Aug 15;25(16):2078-9. doi: 10.1093/bioinformatics/btp352. Epub 2009 Jun 8. PubMed PMID: 19505943; PubMed Central PMCID: PMC2723002.</p> <ul> <li>Somalier</li> </ul> <p>Lee S, Lee S, Ouellette S, Park WY, Lee EA, Park PJ. NGSCheckMate: software for validating sample identity in next-generation sequencing studies within and across data types. Nucleic Acids Res. 2017 Jun 20;45(11):e103. doi: 10.1093/nar/gkx193. PMID: 28369524; PMCID: PMC5499645.</p> <ul> <li>Tabix</li> </ul> <p>Li H, Tabix: fast retrieval of sequence features from generic TAB-delimited files, Bioinformatics, Volume 27, Issue 5, 1 March 2011, Pages 718\u2013719, doi: 10.1093/bioinformatics/btq671. PubMed PMID: 21208982. PubMed Central PMCID: PMC3042176.</p> <ul> <li>UPDio</li> </ul> <p>King DA, Fitzgerald TW, Miller R, Canham N, Clayton-Smith J, Johnson D, Mansour S, Stewart F, Vasudevan P, Hurles ME; DDD Study. A novel method for detecting uniparental disomy from trio genotypes identifies a significant excess in children with developmental disorders. Genome Res. 2014 Apr;24(4):673-87. doi: 10.1101/gr.160465.113. Epub 2013 Dec 19. PMID: 24356988; PMCID: PMC3975066.</p> <ul> <li>Vardict</li> </ul> <p>Zhongwu Lai, Aleksandra Markovets, Miika Ahdesmaki, Brad Chapman, Oliver Hofmann, Robert McEwen, Justin Johnson, Brian Dougherty, J. Carl Barrett, Jonathan R. Dry, VarDict: a novel and versatile variant caller for next-generation sequencing in cancer research, Nucleic Acids Research, Volume 44, Issue 11, 20 June 2016, Page e108, https://doi.org/10.1093/nar/gkw227</p> <ul> <li> <p>VCF2DB</p> </li> <li> <p>VCFanno</p> </li> </ul> <p>Pedersen, B.S., Layer, R.M. &amp; Quinlan, A.R. Vcfanno: fast, flexible annotation of genetic variants. Genome Biol 17, 118 (2016). https://doi.org/10.1186/s13059-016-0973-5</p>"},{"location":"output/","title":"nf-cmgg/smallvariants: Output","text":""},{"location":"output/#introduction","title":"Introduction","text":"<p>This page describes the output produced by the pipeline.</p> <p>The output directory has been structured in such a way that you can pass the same output directory to it for each pipeline run. The pipeline will add the files to that directory in a traceable way without overwriting already existing files. This makes it easy to store data, coming from multiple sequencing runs, in the same root directory.</p> <p>To explain the structure of the output directory, a simple example run consisting of two families is used. The first family (<code>family1</code>) is a family consisting of a trio (son, father and mother) and the second family (<code>family2</code>) consists of a single sample.</p> <pre><code>&lt;outdir&gt; #(1)!\n\u251c\u2500\u2500 family1 #(2)!\n\u2502   \u251c\u2500\u2500 output_&lt;pipeline_version&gt;_&lt;date&gt; #(3)!\n\u2502   \u2502   \u251c\u2500\u2500 automap #(4)!\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 &lt;caller&gt; #(5)!\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 sample1 #(6)!\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 sample1.HomRegions.&lt;panel&gt;.tsv\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 sample1.HomRegions.pdf\n\u2502   \u2502   \u2502       \u2502   \u251c\u2500\u2500 sample1.HomRegions.strict.&lt;panel&gt;.tsv\n\u2502   \u2502   \u2502       \u2502   \u2514\u2500\u2500 sample1.HomRegions.tsv\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 sample2\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 sample3\n\u2502   \u2502   \u251c\u2500\u2500 family1.&lt;caller&gt;.bed #(7)!\n\u2502   \u2502   \u251c\u2500\u2500 family1.&lt;caller&gt;.db #(8)!\n\u2502   \u2502   \u251c\u2500\u2500 family1.&lt;caller&gt;.ped #(9)!\n\u2502   \u2502   \u251c\u2500\u2500 family1.&lt;caller&gt;.vcf.gz #(10)!\n\u2502   \u2502   \u2514\u2500\u2500 family1.&lt;caller&gt;.vcf.gz.tbi #(11)!\n\u2502   \u251c\u2500\u2500 qc_&lt;pipeline_version&gt;_&lt;date&gt; #(12)!\n\u2502   \u2502   \u251c\u2500\u2500 family1.&lt;caller&gt;.bcftools_stats.txt #(13)!\n\u2502   \u2502   \u2514\u2500\u2500 family1.&lt;caller&gt;.html #(14)!\n\u2502   \u251c\u2500\u2500 sample1_&lt;pipeline_version&gt;_&lt;date&gt; #(15)!\n\u2502   \u2502   \u251c\u2500\u2500 sample1.bed #(16)!\n\u2502   \u2502   \u251c\u2500\u2500 sample1.&lt;caller&gt;.bcftools_stats.txt #(17)!\n\u2502   \u2502   \u251c\u2500\u2500 sample1.&lt;caller&gt;.g.vcf.gz #(18)!\n\u2502   \u2502   \u251c\u2500\u2500 sample1.&lt;caller&gt;.g.vcf.gz.tbi #(19)!\n\u2502   \u2502   \u251c\u2500\u2500 sample1.per-base.bed.gz #(33)!\n\u2502   \u2502   \u251c\u2500\u2500 sample1.per-base.bed.gz.csi #(34)!\n\u2502   \u2502   \u2514\u2500\u2500 validation #(20)!\n\u2502   \u2502       \u2514\u2500\u2500 &lt;caller&gt; #(21)!\n\u2502   \u2502           \u251c\u2500\u2500 ... #(22)!\n\u2502   \u2502           \u2514\u2500\u2500 sample1.summary.txt #(23)!\n\u2502   \u251c\u2500\u2500 sample2_&lt;pipeline_version&gt;_&lt;date&gt;\n\u2502   \u2514\u2500\u2500 sample3_&lt;pipeline_version&gt;_&lt;date&gt;\n\u251c\u2500\u2500 family2\n\u2502   \u251c\u2500\u2500 output_&lt;pipeline_version&gt;_&lt;date&gt;\n\u2502   \u251c\u2500\u2500 qc_&lt;pipeline_version&gt;_&lt;date&gt;\n\u2502   \u2514\u2500\u2500 sample4_&lt;pipeline_version&gt;_&lt;date&gt;\n\u2514\u2500\u2500 &lt;pipeline_version&gt;_&lt;date&gt; #(24)!\n    \u251c\u2500\u2500 execution_report_&lt;date&gt;_&lt;hour&gt;-&lt;minutes&gt;-&lt;seconds&gt;.html #(25)!\n    \u251c\u2500\u2500 execution_timeline_&lt;date&gt;_&lt;hour&gt;-&lt;minutes&gt;-&lt;seconds&gt;.html #(26)!\n    \u251c\u2500\u2500 execution_trace_&lt;date&gt;_&lt;hour&gt;-&lt;minutes&gt;-&lt;seconds&gt;.html #(27)!\n    \u251c\u2500\u2500 multiqc_report.html #(28)!\n    \u251c\u2500\u2500 params_2024-11-18_15-41-14.json #(29)!\n    \u251c\u2500\u2500 pipeline_dag_&lt;date&gt;_&lt;hour&gt;-&lt;minutes&gt;-&lt;seconds&gt;.html #(30)!\n    \u251c\u2500\u2500 pipeline_software_mqc_versions.yml #(31)!\n    \u2514\u2500\u2500 samplesheet.&lt;extension&gt; #(32)!\n</code></pre> <ol> <li> <p>The output directory specified with <code>--outdir</code></p> </li> <li> <p>The first family name specified in the samplesheet in the <code>family</code> field</p> </li> <li> <p>This folder contains all major outputs of the current family</p> </li> <li> <p>This folder will only be made when the <code>--automap</code> parameter has been used. It contains all output files from the automap process</p> </li> <li> <p>A specific folder containing postprocessing output generated for the caller used. This folder will be created for each caller provided to the <code>--callers</code> parameter</p> </li> <li> <p>This folder contains the files for the specified sample</p> </li> <li> <p>The BED file used to create the VCF file in this folder using the caller specified in the filename</p> </li> <li> <p>The Gemini DB file generated from the output VCF and the PED file. This file will only be created when <code>--gemini</code> has been used</p> </li> <li> <p>The PED file for the current family. This file will contain the correct samples from the input PED file, when given. The pipeline will try and infer a PED file automatically when none has been given. Mind that the inferring of the PED file can have some issues and isn't perfect. Giving a PED file is the recommended way of providing relational data to the pipeline</p> </li> <li> <p>The final VCF file created using the caller specified in the filename. All required postprocessing methods have been applied on this file</p> </li> <li> <p>The index of the final VCF file</p> </li> <li> <p>This folder contains all quality metrics for the family</p> </li> <li> <p>The statistics calculated by <code>bcftools stats</code></p> </li> <li> <p>The relational report created by <code>somalier relate</code></p> </li> <li> <p>The folder containing sample specific files</p> </li> <li> <p>The BED file used to create the GVCF files for the sample</p> </li> <li> <p>The statistics of the GVCF file, calculate by <code>bcftools stats</code></p> </li> <li> <p>The GVCF file generated by the specified caller</p> </li> <li> <p>The index of the GVCF file</p> </li> <li> <p>This folder contains the validation metrics of this specific sample in the final VCF</p> </li> <li> <p>This folder contains the validation metrics for the final VCF generated using the specified caller</p> </li> <li> <p>Additional files were removed from this example, but they are several VCF files and images for deeper analysis of the validation</p> </li> <li> <p>This file contains a summary of the validation metrics</p> </li> <li> <p>This folder contains pipeline metrics and other pipeline run specific files</p> </li> <li> <p>This file is an HTML file that summarizes a lot of metrics of the pipeline run (cpu usage, memory usage, walltime...)</p> </li> <li> <p>This file is an HTML file that visualizes the timeline of the pipeline run</p> </li> <li> <p>This file is an HTML file that visualizes the trace of the pipeline run</p> </li> <li> <p>The multiqc report containing all main statistics of the output data and tool versions</p> </li> <li> <p>A JSON file containing the used parameters to run this pipeline run</p> </li> <li> <p>This file is an HTML file that visualizes the DAG of the pipeline run</p> </li> <li> <p>This file contains a list of all tools used in the pipeline and their versions</p> </li> <li> <p>The samplesheet used to run this pipeline run</p> </li> <li> <p>The per-base coverage BED file generated by Mosdepth</p> </li> <li> <p>The index of the per-base coverage BED file generated by Mosdepth</p> </li> </ol>"},{"location":"parameters/","title":"nf-cmgg/smallvariants pipeline parameters","text":"<p>A nextflow pipeline for calling and annotating small variants from short DNA reads for WES and WGS data</p>"},{"location":"parameters/#inputoutput-options","title":"Input/output options","text":"<p>Define where the pipeline should find input data and save output data.</p> Parameter Description Type Default Required Hidden <code>input</code> Path to comma-separated file containing information about the samples in the experiment. HelpYou will need to create a design file with information about the samples in your experiment before running the pipeline. Use this parameter to specify its location. It has to be a comma-separated file with samples, and a header row. See usage docs. <code>string</code> True <code>outdir</code> The output directory where the results will be saved. You have to use absolute paths to storage on Cloud infrastructure. <code>string</code> True <code>watchdir</code> A folder to watch for the creation of files that start with <code>watch:</code> in the samplesheet. <code>string</code> <code>email</code> Email address for completion summary. HelpSet this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits. If set in your user config file (<code>~/.nextflow/config</code>) then you don't need to specify this on the command line for every run. <code>string</code> <code>ped</code> Path to a pedigree file for all samples in the run. All relational data will be fetched from this file. <code>string</code>"},{"location":"parameters/#reference-genome-options","title":"Reference genome options","text":"<p>Reference genome related files and options required for the workflow.</p> Parameter Description Type Default Required Hidden <code>genome</code> Reference genome build. Used to fetch the right reference files. HelpRequires a Genome Reference Consortium reference ID (e.g. GRCh38) <code>string</code> GRCh38 <code>fasta</code> Path to FASTA genome file. HelpThis parameter is mandatory if <code>--genome</code> is not specified. The path to the reference genome fasta. <code>string</code> True <code>fai</code> Path to FASTA genome index file. <code>string</code> <code>dict</code> Path to the sequence dictionary generated from the FASTA reference. This is only used when <code>haplotypecaller</code> is one of the specified callers. <code>string</code> <code>strtablefile</code> Path to the STR table file generated from the FASTA reference. This is only used when <code>--dragstr</code> has been given. <code>string</code> <code>sdf</code> Path to the SDF folder generated from the reference FASTA file. This is only required when using <code>--validate</code>. <code>string</code> <code>elfasta</code> Path to the ELFASTA genome file. This is used when <code>elprep</code> is part of the callers and will be automatically generated when missing. <code>string</code> <code>elsites</code> Path to the elsites file. This is used when <code>elprep</code> is part of the callers. <code>string</code> <code>genomes</code> Object for genomes <code>object</code> True <code>genomes_base</code> Directory base for CMGG reference store (used when <code>--genomes_ignore false</code> is specified) <code>string</code> /references/ <code>cmgg_config_base</code> The base directory for the local config files <code>string</code> /conf/ True <code>genomes_ignore</code> Do not load the local references from the path specified with <code>--genomes_base</code> <code>boolean</code> True <code>igenomes_base</code> Directory / URL base for iGenomes references. <code>string</code> True <code>igenomes_ignore</code> Do not load the iGenomes reference config. HelpDo not load <code>igenomes.config</code> when running the pipeline. You may choose this option if you observe clashes between custom parameters and those supplied in <code>igenomes.config</code>. <code>boolean</code> True"},{"location":"parameters/#pipeline-specific-parameters","title":"Pipeline specific parameters","text":"<p>Parameters that define how the pipeline works</p> Parameter Description Type Default Required Hidden <code>scatter_count</code> The amount of scattering that should happen per sample. HelpIncrease this number to increase the pipeline run speed, but at the tradeoff of using more IO and disk space. This can differ from the actual scatter count in some cases (especially with smaller files).This has an effect on HaplotypeCaller, GenomicsDBImport and GenotypeGVCFs. <code>integer</code> 40 <code>merge_distance</code> The merge distance for family BED files HelpIncrease this parameter if GenomicsDBImport is running slow. This defines the maximum distance between intervals that should be merged. The less intervals GenomicsDBImport actually gets, the faster it will run. <code>integer</code> 100000 <code>dragstr</code> Create DragSTR models to be used with HaplotypeCaller HelpThis currently is only able to run single-core per sample. Due to this, the process is very slow with only very small improvements to the analysis. <code>boolean</code> <code>validate</code> Validate the found variants <code>boolean</code> <code>filter</code> Filter the found variants. <code>boolean</code> <code>annotate</code> Annotate the found variants using Ensembl VEP. <code>boolean</code> <code>add_ped</code> Add PED INFO header lines to the final VCFs. <code>boolean</code> <code>gemini</code> Create a Gemini databases from the final VCFs. <code>boolean</code> <code>mosdepth_slow</code> Don't run mosdepth in fast-mode HelpThis is advised if you need exact coverage BED files as output. <code>boolean</code> <code>roi</code> Path to the default ROI (regions of interest) BED file to be used for WES analysis. HelpThis will be used for all samples that do not have a specific ROI file supplied to them through the samplesheet. Don't supply an ROI file to run the analysis as WGS. <code>string</code> <code>dbsnp</code> Path to the dbSNP VCF file. This will be used to set the variant IDs. <code>string</code> <code>dbsnp_tbi</code> Path to the index of the dbSNP VCF file. <code>string</code> <code>somalier_sites</code> Path to the VCF file with sites for Somalier to use. <code>string</code> https://github.com/brentp/somalier/files/3412456/sites.hg38.vcf.gz <code>only_call</code> Only call the variants without doing any post-processing. <code>boolean</code> <code>only_merge</code> Only run the pipeline until the creation of the genomicsdbs and output them. <code>boolean</code> <code>output_genomicsdb</code> Output the genomicsDB together with the joint-genotyped VCF. <code>boolean</code> <code>callers</code> A comma delimited string of the available callers. Current options are: <code>haplotypecaller</code> and <code>vardict</code>. <code>string</code> haplotypecaller <code>vardict_min_af</code> The minimum allele frequency for VarDict when no <code>vardict_min_af</code> is supplied in the samplesheet. <code>number</code> 0.1 <code>normalize</code> Normalize the variant in the final VCFs. <code>boolean</code> <code>only_pass</code> Filter out all variants that don't have the PASS filter for vardict. This only works when <code>--filter</code> is also given. <code>boolean</code> <code>keep_alt_contigs</code> Keep all aditional contigs for calling instead of filtering them out before. <code>boolean</code> <code>updio</code> Run UPDio analysis on the final VCFs. <code>boolean</code> <code>updio_common_cnvs</code> A TSV file containing common CNVs to be used by UPDio. <code>string</code> <code>automap</code> Run AutoMap analysis on the final VCFs. <code>boolean</code> <code>automap_repeats</code> BED file with repeat regions in the genome. HelpThis file will be automatically generated for hg38/GRCh38 and hg19/GRCh37 when this parameter has not been given. <code>string</code> <code>automap_panel</code> TXT file with gene panel regions to be used by AutoMap. HelpBy default the CMGG gene panel list will be used. <code>string</code> <code>automap_panel_name</code> The panel name of the panel given with --automap_panel. <code>string</code> cmgg_bio <code>hc_phasing</code> Perform phasing with HaplotypeCaller. <code>boolean</code> <code>min_callable_coverage</code> The lowest callable coverage to determine callable regions. <code>integer</code> 5 <code>unique_out</code> Don't change this value <code>string</code> True <code>disable_hc_dict_validation</code> Disable the sequence dictionary validation in HaplotypeCaller <code>boolean</code> <code>skip_merged_cram_output</code> Don't output the merged CRAM files. <code>boolean</code>"},{"location":"parameters/#institutional-config-options","title":"Institutional config options","text":"<p>Parameters used to describe centralised config profiles. These should not be edited.</p> Parameter Description Type Default Required Hidden <code>custom_config_version</code> Git commit id for Institutional configs. <code>string</code> master True <code>custom_config_base</code> Base directory for Institutional configs. HelpIf you're running offline, Nextflow will not be able to fetch the institutional config files from the internet. If you don't need them, then this is not a problem. If you do need them, you should download the files from the repo and tell Nextflow where to find them with this parameter. <code>string</code> https://raw.githubusercontent.com/nf-core/configs/master True <code>config_profile_name</code> Institutional config name. <code>string</code> True <code>config_profile_description</code> Institutional config description. <code>string</code> True <code>config_profile_contact</code> Institutional config contact information. <code>string</code> True <code>config_profile_url</code> Institutional config URL link. <code>string</code> True"},{"location":"parameters/#generic-options","title":"Generic options","text":"<p>Less common options for the pipeline, typically set in a config file.</p> Parameter Description Type Default Required Hidden <code>version</code> Display version and exit. <code>boolean</code> <code>publish_dir_mode</code> Method used to save pipeline results to output directory. HelpThe Nextflow <code>publishDir</code> option specifies which intermediate files should be saved to the output directory. This option tells the pipeline what method should be used to move these files. See Nextflow docs for details. <code>string</code> copy <code>email_on_fail</code> Email address for completion summary, only when pipeline fails. HelpAn email address to send a summary email to when the pipeline is completed - ONLY sent if the pipeline does not exit successfully. <code>string</code> True <code>plaintext_email</code> Send plain-text email instead of HTML. <code>boolean</code> True <code>max_multiqc_email_size</code> File size limit when attaching MultiQC reports to summary emails. <code>string</code> 25.MB True <code>monochrome_logs</code> Do not use coloured log outputs. <code>boolean</code> True <code>hook_url</code> Incoming hook URL for messaging service HelpIncoming hook URL for messaging service. Currently, MS Teams and Slack are supported. <code>string</code> <code>multiqc_title</code> MultiQC report title. Printed as page header, used for filename if not otherwise specified. <code>string</code> <code>multiqc_config</code> Custom config file to supply to MultiQC. <code>string</code> <code>multiqc_logo</code> Custom logo file to supply to MultiQC. File name must also be set in the MultiQC config file <code>string</code> <code>multiqc_methods_description</code> Custom MultiQC yaml file containing HTML including a methods description. <code>string</code> <code>validate_params</code> Boolean whether to validate parameters against the schema at runtime <code>boolean</code> True True <code>pipelines_testdata_base_path</code> Base URL or local path to location of pipeline test dataset files <code>string</code> https://raw.githubusercontent.com/nf-core/test-datasets/ True"},{"location":"parameters/#annotation-parameters","title":"Annotation parameters","text":"<p>Parameters to configure Ensembl VEP and VCFanno</p> Parameter Description Type Default Required Hidden <code>vep_chunk_size</code> The amount of sites per split VCF as input to VEP. <code>integer</code> 50000 <code>species</code> The species of the samples. HelpMust be lower case and have underscores as spaces. <code>string</code> homo_sapiens <code>vep_merged</code> Specify if the VEP cache is a merged cache. <code>boolean</code> True <code>vep_cache</code> The path to the VEP cache. <code>string</code> <code>vep_dbnsfp</code> Use the dbNSFP plugin with Ensembl VEP. HelpThe '--dbnsfp' and '--dbnsfp_tbi' parameters need to be specified when using this parameter. <code>boolean</code> <code>vep_spliceai</code> Use the SpliceAI plugin with Ensembl VEP. HelpThe '--spliceai_indel', '--spliceai_indel_tbi', '--spliceai_snv' and '--spliceai_snv_tbi' parameters need to be specified when using this parameter. <code>boolean</code> <code>vep_spliceregion</code> Use the SpliceRegion plugin with Ensembl VEP. <code>boolean</code> <code>vep_mastermind</code> Use the Mastermind plugin with Ensembl VEP. HelpThe '--mastermind' and '--mastermind_tbi' parameters need to be specified when using this parameter. <code>boolean</code> <code>vep_maxentscan</code> Use the MaxEntScan plugin with Ensembl VEP. HelpThe '--maxentscan' parameter need to be specified when using this parameter. <code>boolean</code> <code>vep_eog</code> Use the custom EOG annotation with Ensembl VEP. HelpThe '--eog' and '--eog_tbi' parameters need to be specified when using this parameter. <code>boolean</code> <code>vep_alphamissense</code> Use the AlphaMissense plugin with Ensembl VEP. HelpThe '--alphamissense' and '--alphamissense_tbi' parameters need to be specified when using this parameter. <code>boolean</code> <code>vep_version</code> The version of the VEP tool to be used. <code>number</code> 105.0 <code>vep_cache_version</code> The version of the VEP cache to be used. <code>integer</code> 105 <code>dbnsfp</code> Path to the dbSNFP file. <code>string</code> <code>dbnsfp_tbi</code> Path to the index of the dbSNFP file. <code>string</code> <code>spliceai_indel</code> Path to the VCF containing indels for spliceAI. <code>string</code> <code>spliceai_indel_tbi</code> Path to the index of the VCF containing indels for spliceAI. <code>string</code> <code>spliceai_snv</code> Path to the VCF containing SNVs for spliceAI. <code>string</code> <code>spliceai_snv_tbi</code> Path to the index of the VCF containing SNVs for spliceAI. <code>string</code> <code>mastermind</code> Path to the VCF for Mastermind. <code>string</code> <code>mastermind_tbi</code> Path to the index of the VCF for Mastermind. <code>string</code> <code>alphamissense</code> Path to the TSV for AlphaMissense. <code>string</code> <code>alphamissense_tbi</code> Path to the index of the TSV for AlphaMissense. <code>string</code> <code>eog</code> Path to the VCF containing EOG annotations. <code>string</code> <code>eog_tbi</code> Path to the index of the VCF containing EOG annotations. <code>string</code> <code>vcfanno</code> Run annotations with vcfanno. <code>boolean</code> <code>vcfanno_config</code> The path to the VCFanno config TOML. <code>string</code> <code>vcfanno_lua</code> The path to a Lua script to be used in VCFanno. <code>string</code> <code>vcfanno_resources</code> A semicolon-seperated list of resource files for VCFanno, please also supply their indices using this parameter. <code>string</code>"},{"location":"usage/","title":"nf-cmgg/smallvariants: Usage","text":"<p>Documentation of pipeline parameters can be found in the parameters documentation</p>"},{"location":"usage/#samplesheet-input","title":"Samplesheet input","text":"<p>You will need to create a samplesheet with information with the samples you would like to analyse before running the pipeline. Use this parameter to specify its location. It can be either a CSV, TSV, JSON or YAML file.</p> <pre><code>--input '[path to samplesheet file]'\n</code></pre>"},{"location":"usage/#watch-for-files-in-a-directory","title":"Watch for files in a directory","text":"<p>When the <code>--watchdir</code> parameter has been given, the pipeline will automatically check for all files in the samplesheet that have the <code>watch:</code> prefix in the given directory. An example for watching CRAM files:</p> samplesheet.csv<pre><code>sample,cram,crai\nSAMPLE_1,watch:INPUT.cram,watch:INPUT.cram.crai\n</code></pre> <p>The files <code>INPUT.cram</code> and <code>INPUT.cram.crai</code> will now be watched for recursively in the watch directory.</p>"},{"location":"usage/#example-of-the-samplesheet","title":"Example of the samplesheet","text":"<p>Below is an example of how the samplesheet could look like in the three formats.</p> <p>Note</p> <p>The order and presence of the fields is not set, you can arrange/remove these as you see fit. The only required fields are <code>sample</code> and <code>cram</code>.</p>"},{"location":"usage/#csv","title":"CSV","text":"samplesheet.csv<pre><code>sample,family,cram,crai\nSAMPLE_1,FAMILY_1,SAMPLE_1.cram,SAMPLE_1.crai\nSAMPLE_2,FAMILY_1,SAMPLE_2.cram,SAMPLE_2.crai\nSAMPLE_3,,SAMPLE_3.cram,\n</code></pre>"},{"location":"usage/#tsv","title":"TSV","text":"samplesheet.tsv<pre><code>sample    family    cram   crai\nSAMPLE_1  FAMILY_1  SAMPLE_1.cram SAMPLE_1.crai\nSAMPLE_2  FAMILY_1  SAMPLE_2.cram SAMPLE_2.crai\nSAMPLE_3    SAMPLE_3.cram\n</code></pre>"},{"location":"usage/#yamlyml","title":"YAML/YML","text":"samplesheet.yaml<pre><code>- sample: SAMPLE_1\n  family: FAMILY_1\n  cram: SAMPLE_1.cram\n  crai: SAMPLE_1.crai\n- sample: SAMPLE_2\n  family: FAMILY_1\n  cram: SAMPLE_2.cram\n  crai: SAMPLE_2.crai\n- sample: SAMPLE_3\n  cram: SAMPLE_3.cram\n</code></pre>"},{"location":"usage/#json","title":"JSON","text":"samplesheet.json<pre><code>[\n  {\n    \"sample\": \"SAMPLE_1\",\n    \"family\": \"FAMILY_1\",\n    \"cram\": \"SAMPLE_1.cram\",\n    \"crai\": \"SAMPLE_1.crai\"\n  },\n  {\n    \"sample\": \"SAMPLE_2\",\n    \"family\": \"FAMILY_1\",\n    \"cram\": \"SAMPLE_2.cram\",\n    \"crai\": \"SAMPLE_2.crai\"\n  },\n  {\n    \"sample\": \"SAMPLE_3\",\n    \"cram\": \"SAMPLE_3.cram\"\n  }\n]\n</code></pre>"},{"location":"usage/#full-samplesheet","title":"Full samplesheet","text":"<p>The samplesheet can have following columns:</p> Column Description <code>sample</code> MANDATORY - Custom sample name. This entry has to be identical for multiple sequencing libraries/runs from the same sample. Spaces in sample names are automatically converted to underscores (<code>_</code>). <code>family</code> OPTIONAL - The family ID of the specified sample. This field is optional, as the family id can also be extracted from the <code>ped</code> file. If no <code>ped</code> file and <code>family</code> ID are supplied, the <code>family</code> ID defaults to the <code>sample</code> ID (which means that the resulting VCF will be single-sample). Spaces in family names are automatically converted to underscores (<code>_</code>). <code>cram</code> MANDATORY - Full path to CRAM file to call variants from. File has to have the extension <code>.cram</code> <code>crai</code> OPTIONAL - Full path to CRAM index file. File has to have the extension <code>.crai</code>. <code>ped</code> OPTIONAL - Full path to PED file containing the relational information between samples in the same family. File has to have the extension <code>.ped</code>. <code>truth_vcf</code> OPTIONAL - Full path to the VCF containing all the truth variants of the current sample. The validation subworkflow will be run when this file is supplied and the <code>--validate true</code> flag has been given. File has to have the extension <code>.vcf.gz</code> <code>truth_tbi</code> OPTIONAL - Full path to the index of the truth VCF. This file can either be supplied by the user or generated by the pipeline. File has to have the extensions <code>.tbi</code> <code>truth_bed</code> OPTIONAL - Full path to the BED file containing the golden truth regions in the <code>truth_vcf</code> file. File has to have the extensions <code>.bed</code> <code>roi</code> OPTIONAL - Full path to a BED file containing the regions of interest for the current sample to call on. When this file is given, the pipeline will run this sample in WES mode. (The flag <code>--roi &lt;path&gt;</code> can also be given to run WES mode for all samples using the file specified by the flag) File has to have the extension <code>.bed</code> or <code>.bed.gz</code>. <code>vardict_min_af</code> OPTIONAL - The minimum AF value to use for the vardict variant caller (<code>--callers vardict</code>). This can be set in the samplesheet when it differs for all samples. A default can be set using the <code>--vardict_min_af</code> parameter (whichs defaults to 0.1) <p>Note</p> <p>The <code>sample</code> fields has to contain the same value when you have re-sequenced the same sample more than once e.g. to increase sequencing depth. Either the <code>ped</code> or <code>family</code> field can be used to specify the family name. The pipeline automatically extracts the family id from the <code>ped</code> file if the <code>family</code> field is empty. The <code>family</code> is used to specify on which samples the joint-genotyping should be performed. If neither the <code>ped</code> or <code>family</code> fields are used, the pipeline will default to a single-sample family with the sample name as its ID.</p> <p>This is an example of a working samplesheet used to test this pipeline:</p> samplesheet.csv<pre><code>sample,family,cram,crai,roi,truth_vcf,truth_tbi,truth_bed,vardict_min_af\nNA24143,Proband_12.345,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/crams/NA24143.cram,,,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/vcfs/NA24143.vcf.gz,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/vcfs/NA24143.vcf.gz.tbi,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/regions/roi.bed,0.01\nNA24149,Proband_12.345,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/crams/NA24149.cram,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/crams/NA24149.cram.crai,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/regions/roi.bed,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/vcfs/NA24149.vcf.gz,,,\nNA24385,Proband_12.345,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/crams/NA24385.cram,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/crams/NA24385.cram.crai,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/regions/roi.bed,https://github.com/nf-cmgg/test-datasets/raw/smallvariants/data/genomics/homo_sapiens/illumina/vcfs/NA24385.vcf.gz,,,\n</code></pre>"},{"location":"usage/#running-the-pipeline","title":"Running the pipeline","text":"<p>The typical command for running the pipeline is as follows:</p> <pre><code>nextflow run nf-cmgg/smallvariants --input ./samplesheet.csv --outdir ./results --genome GRCh38 -profile docker\n</code></pre> <p>This will launch the pipeline with the <code>docker</code> configuration profile. See below for more information about profiles.</p> <p>Note that the pipeline will create the following files in your working directory:</p> <pre><code>work          #(1)!\nresults       #(2)!\n.nextflow.log #(3)!\n...           #(4)!\n</code></pre> <ol> <li> <p>Directory containing the nextflow working files</p> </li> <li> <p>Finished results in specified location (defined with --outdir). See output documentation for more on this.</p> </li> <li> <p>Log file from Nextflow</p> </li> <li> <p>Other nextflow hidden files, eg. history of pipeline runs and old logs.</p> </li> </ol> <p>If you wish to repeatedly use the same parameters for multiple runs, rather than specifying each flag in the command, you can specify these in a params file.</p> <p>Pipeline settings can be provided in a <code>yaml</code> or <code>json</code> file via <code>-params-file &lt;file&gt;</code>.</p> <p>Warning</p> <p>Do not use <code>-c &lt;file&gt;</code> to specify parameters as this will result in errors. Custom config files specified with <code>-c</code> must only be used for tuning process resource specifications, other infrastructural tweaks (such as output directories), or module arguments (args).</p> <p>The above pipeline run specified with a params file in yaml format:</p> <pre><code>nextflow run nf-cmgg/smallvariants -profile docker -params-file params.yaml\n</code></pre> <p>with:</p> params.yaml<pre><code>input: './samplesheet.csv'\noutdir: './results/'\ngenome: 'GRCh38'\n&lt;...&gt;\n</code></pre>"},{"location":"usage/#updating-the-pipeline","title":"Updating the pipeline","text":"<p>When you run the above command, Nextflow automatically pulls the pipeline code from GitHub and stores it as a cached version. When running the pipeline after this, it will always use the cached version if available - even if the pipeline has been updated since. To make sure that you're running the latest version of the pipeline, make sure that you regularly update the cached version of the pipeline. You can also add the <code>-latest</code> argument to your run command to automatically fetch the latest version on every run:</p> <pre><code>nextflow pull nf-cmgg/smallvariants -r &lt;version&gt;\n</code></pre>"},{"location":"usage/#reproducibility","title":"Reproducibility","text":"<p>It is a good idea to specify a pipeline version when running the pipeline on your data. This ensures that a specific version of the pipeline code and software are used when you run your pipeline. If you keep using the same tag, you'll be running the same version of the pipeline, even if there have been changes to the code since.</p> <p>First, go to the nf-cmgg/smallvariants releases page and find the latest pipeline version - numeric only (eg. <code>1.3.1</code>). Then specify this when running the pipeline with <code>-r</code> (one hyphen) - eg. <code>-r 1.3.1</code>. Of course, you can switch to another version by changing the number after the <code>-r</code> flag.</p> <p>This version number will be logged in reports when you run the pipeline, so that you'll know what you used when you look back in the future. For example, at the bottom of the MultiQC reports.</p> <p>To further assist in reproducbility, you can use share and re-use parameter files to repeat pipeline runs with the same settings without having to write out a command with every single parameter.</p> <p>Tip</p> <p>If you wish to share such profile (such as upload as supplementary material for academic publications), make sure to NOT include cluster specific paths to files, nor institutional specific profiles.</p>"},{"location":"usage/#core-nextflow-arguments","title":"Core Nextflow arguments","text":"<p>Note</p> <p>These options are part of Nextflow and use a single hyphen (pipeline parameters use a double-hyphen).</p>"},{"location":"usage/#-profile","title":"<code>-profile</code>","text":"<p>Use this parameter to choose a configuration profile. Profiles can give configuration presets for different compute environments.</p> <p>Several generic profiles are bundled with the pipeline which instruct the pipeline to use software packaged using different methods (Docker, Singularity, Podman, Shifter, Charliecloud, Apptainer, Conda) - see below. Several generic profiles are bundled with the pipeline which instruct the pipeline to use software packaged using different methods (Docker, Singularity, Podman, Shifter, Charliecloud, Apptainer, Conda) - see below.</p> <p>Info</p> <p>We highly recommend the use of Docker or Singularity containers for full pipeline reproducibility, however when this is not possible, Conda is also supported.</p> <p>The pipeline also dynamically loads configurations from https://github.com/nf-core/configs when it runs, making multiple config profiles for various institutional clusters available at run time. For more information and to see if your system is available in these configs please see the nf-core/configs documentation.</p> <p>Note that multiple profiles can be loaded, for example: <code>-profile test,docker</code> - the order of arguments is important! They are loaded in sequence, so later profiles can overwrite earlier profiles.</p> <p>If <code>-profile</code> is not specified, the pipeline will run locally and expect all software to be installed and available on the <code>PATH</code>. This is not recommended, since it can lead to different results on different machines dependent on the computer enviroment.</p> <ul> <li><code>test</code> <p>A profile with a complete configuration for automated testing Includes links to test data so needs no other parameters</p> </li> <li><code>nf_test</code> <p>The profile setting the default values for <code>nf-test</code>. When running <code>nf-test</code> this profile is automatically used.</p> </li> <li><code>docker</code> <p>A generic configuration profile to be used with Docker</p> </li> <li><code>singularity</code> <p>A generic configuration profile to be used with Singularity</p> </li> <li><code>podman</code> <p>A generic configuration profile to be used with Podman</p> </li> <li><code>shifter</code> <p>A generic configuration profile to be used with Shifter</p> </li> <li><code>charliecloud</code> <p>A generic configuration profile to be used with Charliecloud</p> </li> <li><code>apptainer</code> <p>A generic configuration profile to be used with Apptainer</p> </li> <li><code>conda</code> <p>A generic configuration profile to be used with Conda. Please only use Conda as a last resort i.e. when it's not possible to run the pipeline with Docker, Singularity, Podman, Shifter, Charliecloud, or Apptainer.</p> </li> </ul>"},{"location":"usage/#-resume","title":"<code>-resume</code>","text":"<p>Specify this when restarting a pipeline. Nextflow will use cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. For input to be considered the same, not only the names must be identical but the files' contents as well. For more info about this parameter, see this blog post.</p> <p>You can also supply a run name to resume a specific run: <code>-resume [run-name]</code>. Use the <code>nextflow log</code> command to show previous run names.</p>"},{"location":"usage/#-c","title":"<code>-c</code>","text":"<p>Specify the path to a specific config file. See the nf-core website documentation for more information.</p>"},{"location":"usage/#custom-configuration","title":"Custom configuration","text":""},{"location":"usage/#resource-requests","title":"Resource requests","text":"<p>Whilst the default requirements set within the pipeline will hopefully work for most people and with most input data, you may find that you want to customise the compute resources that the pipeline requests. Each step in the pipeline has a default set of requirements for number of CPUs, memory and time. For most of the steps in the pipeline, if the job exits with any of the error codes specified here it will automatically be resubmitted with higher requests (2 x original, then 3 x original). If it still fails after the third attempt then the pipeline execution is stopped.</p> <p>To change the resource requests, please see the max resources and tuning workflow resources section of the nf-core website.</p>"},{"location":"usage/#custom-containers","title":"Custom Containers","text":"<p>In some cases you may wish to change which container or conda environment a step of the pipeline uses for a particular tool. By default nf-core pipelines use containers and software from the biocontainers or bioconda projects. However in some cases the pipeline specified version maybe out of date.</p> <p>To use a different container from the default container or conda environment specified in a pipeline, please see the updating tool versions section of the nf-core website.</p>"},{"location":"usage/#custom-tool-arguments","title":"Custom Tool Arguments","text":"<p>A pipeline might not always support every possible argument or option of a particular tool used in pipeline. Fortunately, nf-core pipelines provide some freedom to users to insert additional parameters that the pipeline does not include by default.</p> <p>To learn how to provide additional arguments to a particular tool of the pipeline, please see the customising tool arguments section of the nf-core website.</p>"},{"location":"usage/#nf-coreconfigs","title":"nf-core/configs","text":"<p>In most cases, you will only need to create a custom config as a one-off but if you and others within your organisation are likely to be running nf-core pipelines regularly and need to use the same settings regularly it may be a good idea to request that your custom config file is uploaded to the <code>nf-core/configs</code> git repository. Before you do this please can you test that the config file works with your pipeline of choice using the <code>-c</code> parameter. You can then create a pull request to the <code>nf-core/configs</code> repository with the addition of your config file, associated documentation file (see examples in <code>nf-core/configs/docs</code>), and amending <code>nfcore_custom.config</code> to include your custom profile.</p> <p>See the main Nextflow documentation for more information about creating your own configuration files.</p> <p>If you have any questions or issues please send us a message on Slack on the <code>#configs</code> channel.</p>"},{"location":"usage/#running-in-the-background","title":"Running in the background","text":"<p>Nextflow handles job submissions and supervises the running jobs. The Nextflow process must run until the pipeline is finished.</p> <p>The Nextflow <code>-bg</code> flag launches Nextflow in the background, detached from your terminal so that the workflow does not stop if you log out of your session. The logs are saved to a file.</p> <p>Alternatively, you can use <code>screen</code> / <code>tmux</code> or similar tool to create a detached session which you can log back into at a later time. Some HPC setups also allow you to run nextflow within a cluster job submitted your job scheduler (from where it submits more jobs).</p>"},{"location":"usage/#nextflow-memory-requirements","title":"Nextflow memory requirements","text":"<p>In some cases, the Nextflow Java virtual machines can start to request a large amount of memory. We recommend adding the following line to your environment to limit this (typically in <code>~/.bashrc</code> or <code>~./bash_profile</code>):</p> <pre><code>NXF_OPTS='-Xms1g -Xmx4g'\n</code></pre>"}]}